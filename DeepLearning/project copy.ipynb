{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt # komoran, han, kkma\n",
    "from jamo import h2j, j2hcj\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Dropout\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 단어 개수:  166\n"
     ]
    }
   ],
   "source": [
    "# 각자 경로에 맞게 수정해주세요\n",
    "train = pd.read_csv(r'data/train_merge.csv')\n",
    "test = pd.read_csv(r'data/test_merge.csv')\n",
    "\n",
    "# 자모음 쪼갠 텍스트를 한글자씩 리스트로 만들어서 데이터셋 만든다\n",
    "def textToList(text):\n",
    "    li = []\n",
    "    for t in text:\n",
    "        li.append(list(t))\n",
    "    return li\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x : j2hcj(h2j(x)))\n",
    "test['text'] = test['text'].apply(lambda x : j2hcj(h2j(x)))\n",
    "\n",
    "X_train = textToList(train['text'])\n",
    "y_train = train['immoral']\n",
    "\n",
    "X_test = textToList(test['text'])\n",
    "y_test = test['immoral']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=110)\n",
    "tokenizer.fit_on_texts(X_train) # 단어 인덱스 구축\n",
    "text_sequences = tokenizer.texts_to_sequences(X_train) # 문자열 -> 인덱스 리스트\n",
    "                                                            # '나는 천재다 나는 멋있다' -> [1, 2, 1, 3]\n",
    "# train data로 fit 시켜준 tokenizer로 test data도 인덱스로 변경\n",
    "text_sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "word_vocab = tokenizer.word_index # 딕셔너리 형태\n",
    "print(\"전체 단어 개수: \", len(word_vocab)) # 전체 단어 개수 확인\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100 # 문장 최대 길이\n",
    "\n",
    "X_train = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='pre')\n",
    "y_train = np.array(y_train) # 각 리뷰의 감정을 넘파이 배열로 만든다.\n",
    "\n",
    "X_test = pad_sequences(text_sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='pre')\n",
    "y_test = np.array(y_test) # 각 리뷰의 감정을 넘파이 배열로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 아래 변수 원하는대로 수정\n",
    "embedding_dim = 100 # 임베딩 벡터의 차원\n",
    "dropout_ratio = 0.3 # 드롭아웃 비율\n",
    "num_filters = 256 # 커널의 수\n",
    "kernel_size = 3 # 커널의 크기\n",
    "hidden_units = 128 # 뉴런의 수\n",
    "####\n",
    "# 모델링 시작\n",
    "## Conv1D 갯수, num_filters 원하는대로 수정\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_vocab)+1, 100))\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "# Pooling 없애도 괜찮습니다\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(X_train, y_train, epochs= 100, batch_size = 100, validation_split=0.2, callbacks=[early])\n",
    "\n",
    "\n",
    "model.save('convModel.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mzp",
   "language": "python",
   "name": "mzp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
