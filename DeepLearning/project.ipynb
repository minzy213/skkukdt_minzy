{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt # komoran, han, kkma\n",
    "from jamo import h2j, j2hcj\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 가져오기, 학습 데이터셋으로 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인벤에서 수집한 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10380\\2151298477.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  immoral = pd.read_csv(r'data\\immoral.txt', sep = 'asssssssss', header = None)\n"
     ]
    }
   ],
   "source": [
    "immoral = pd.read_csv(r'data\\immoral.txt', sep = 'asssssssss', header = None)\n",
    "immoral.columns = ['text']\n",
    "immoral['immoral'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10380\\2827705178.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  clean = pd.read_csv(r'data\\clean.txt', sep = 'asssssssss', header = None)\n"
     ]
    }
   ],
   "source": [
    "clean = pd.read_csv(r'data\\clean.txt', sep = 'asssssssss', header = None)\n",
    "clean.columns = ['text']\n",
    "clean['immoral'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## op.gg 데이터 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lii = []\n",
    "headers = {\n",
    "    'Referer':'https://www.op.gg/',\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\n",
    "}\n",
    "for chm in champ:\n",
    "    n = 1\n",
    "    while 1:\n",
    "        url = f'https://op.gg/api/v1.0/internal/bypass/champions/{chm}/comments?&sort=popular&page={n}&limit=100&hl=ko_KR&is_latest_version=false'\n",
    "        n += 1\n",
    "        resp = requests.get(url, headers = headers)\n",
    "        dict_ch = json.loads(resp.text)\n",
    "        if len(dict_ch['data']) == 0:\n",
    "            break\n",
    "        \n",
    "        for dat in dict_ch['data']:\n",
    "            lii.append(dat['content'].replace('\\n', ''))\n",
    "        time.sleep(5)\n",
    "    \n",
    "    print(chm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>immoral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>얘 와맆에 곰돌이가 에어본 시키는 거 생겼던데 여기에도 생기나</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>자기고향 찾아오네</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>몇달만의 고향이냐</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>점멸 있을 때 한정 1티어</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>스킬 사거리 표시 없이 하면 암걸리는 챔</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7968</th>\n",
       "      <td>의외로 꼴짤이 별로 없음</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7969</th>\n",
       "      <td>나피리 근데 초반 라인전도 약하고 카운터도 많음.그리고 돌진기가 다른 암살자와 다르...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7970</th>\n",
       "      <td>0/6/0 야스오:야이 개쌔꺄 이게 챔이냐 야발4/0/2 나피리:개 맞는데0/8/0...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7971</th>\n",
       "      <td>얘 한테 지면 사람아닌래끼다버프좀 하고 성능좀 버프히라못 쓸 정도다</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7972</th>\n",
       "      <td>버프좀 해줘</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7973 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text immoral\n",
       "0                    얘 와맆에 곰돌이가 에어본 시키는 거 생겼던데 여기에도 생기나        \n",
       "1                                             자기고향 찾아오네        \n",
       "2                                             몇달만의 고향이냐        \n",
       "3                                        점멸 있을 때 한정 1티어        \n",
       "4                                스킬 사거리 표시 없이 하면 암걸리는 챔        \n",
       "...                                                 ...     ...\n",
       "7968                                      의외로 꼴짤이 별로 없음        \n",
       "7969  나피리 근데 초반 라인전도 약하고 카운터도 많음.그리고 돌진기가 다른 암살자와 다르...        \n",
       "7970  0/6/0 야스오:야이 개쌔꺄 이게 챔이냐 야발4/0/2 나피리:개 맞는데0/8/0...        \n",
       "7971              얘 한테 지면 사람아닌래끼다버프좀 하고 성능좀 버프히라못 쓸 정도다        \n",
       "7972                                             버프좀 해줘        \n",
       "\n",
       "[7973 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lii = pd.DataFrame(lii, columns = ['text'])\n",
    "df_lii['immoral'] = ''\n",
    "df_lii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### op.gg 데이터 직접 라벨링, 라벨링 된 csv 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lii.to_csv(r'data/champions.csv', index = False)\n",
    "df_lii = pd.read_csv(r'data/champions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10380\\2204739627.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'False' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  champions.loc[champions['immoral'] == 0, 'immoral'] = False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>immoral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>얘 와맆에 곰돌이가 에어본 시키는 거 생겼던데 여기에도 생기나</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>자기고향 찾아오네</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>몇달만의 고향이냐</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>점멸 있을 때 한정 1티어</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>스킬 사거리 표시 없이 하면 암걸리는 챔</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6458</th>\n",
       "      <td>클레드로 굶드라랑 월식 못 가게 너프먹는다니까 진짜 ㅈ같아서 겜 못해먹겠네 월식은 ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6459</th>\n",
       "      <td>진짜 개사기임아니 협곡말고 롤체 학살자 클레드요</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6460</th>\n",
       "      <td>진짜 태불방 롤백 시킬거 아니면 클레드는 건들면 안되지 ㅋㅋㅋ 선넘네 진짜</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6461</th>\n",
       "      <td>\"바텀에서 겨우 더블킬을 했다.\"\"나팔소리가 들렸다.\"\"여름이었다.\"(제압되었습니다!)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6462</th>\n",
       "      <td>ㄹㅇ 누군지도 몰랐다가 호되게 당함;;;</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6463 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text immoral\n",
       "0                    얘 와맆에 곰돌이가 에어본 시키는 거 생겼던데 여기에도 생기나   False\n",
       "1                                             자기고향 찾아오네   False\n",
       "2                                             몇달만의 고향이냐   False\n",
       "3                                        점멸 있을 때 한정 1티어   False\n",
       "4                                스킬 사거리 표시 없이 하면 암걸리는 챔   False\n",
       "...                                                 ...     ...\n",
       "6458  클레드로 굶드라랑 월식 못 가게 너프먹는다니까 진짜 ㅈ같아서 겜 못해먹겠네 월식은 ...    True\n",
       "6459                         진짜 개사기임아니 협곡말고 롤체 학살자 클레드요   False\n",
       "6460          진짜 태불방 롤백 시킬거 아니면 클레드는 건들면 안되지 ㅋㅋㅋ 선넘네 진짜   False\n",
       "6461   \"바텀에서 겨우 더블킬을 했다.\"\"나팔소리가 들렸다.\"\"여름이었다.\"(제압되었습니다!)   False\n",
       "6462                             ㄹㅇ 누군지도 몰랐다가 호되게 당함;;;   False\n",
       "\n",
       "[6463 rows x 2 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "champions = pd.read_csv(r'data\\champions.csv')\n",
    "champions.columns = ['text','immoral']\n",
    "champions.loc[champions['immoral'] == 0, 'immoral'] = False\n",
    "champions.loc[champions['immoral'] == 1, 'immoral'] = True\n",
    "champions = champions.fillna(False)\n",
    "champions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ai hub 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai hub train set\n",
    "li = []\n",
    "for i in range(1, 6):\n",
    "    with open(r'.\\data\\train\\labled\\talksets-train-' + str(i) + '.json') as f:\n",
    "        data = json.load(f)\n",
    "    for d in data:\n",
    "        for s in d['sentences']:\n",
    "            li.append([s['origin_text'], s['is_immoral'], s['intensity'], \n",
    "                    'CENSURE' in s['types'], 'HATE' in s['types'], 'DISCRIMINATION' in s['types'], \n",
    "                    'SEXUAL' in s['types'], 'ABUSE' in s['types'], 'VIOLENCE' in s['types'], 'CRIME' in s['types']])\n",
    "            \n",
    "train = pd.DataFrame(li, columns=['text', 'immoral', 'intensity', 'CENSURE', 'HATE', 'DISCRIMINATION', 'SEXUAL', 'ABUSE', 'VIOLENCE', 'CRIME'])\n",
    "train['text'] = train['text'].apply(lambda x : re.sub('#@.+?#', '', x.replace('#@#', '')))\n",
    "\n",
    "train['immoral'] = (train['SEXUAL'] == True) | (train['ABUSE'] == True)\n",
    "train_set = train[['text', 'immoral']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai hub test set\n",
    "li = []\n",
    "with open(r'.\\data\\valid\\labled\\talksets-train-6.json') as f:\n",
    "    data = json.load(f)\n",
    "for d in data:\n",
    "    for s in d['sentences']:\n",
    "        li.append([s['origin_text'], s['is_immoral'], s['intensity'], \n",
    "                'CENSURE' in s['types'], 'HATE' in s['types'], 'DISCRIMINATION' in s['types'], \n",
    "                'SEXUAL' in s['types'], 'ABUSE' in s['types'], 'VIOLENCE' in s['types'], 'CRIME' in s['types']])\n",
    "test = pd.DataFrame(li, columns=['text', 'immoral', 'intensity', 'CENSURE', 'HATE', 'DISCRIMINATION', 'SEXUAL', 'ABUSE', 'VIOLENCE', 'CRIME'])\n",
    "\n",
    "test['text'] = test['text'].apply(lambda x : re.sub('#@.+?#', '', x.replace('#@#', '')))\n",
    "\n",
    "test['immoral'] = (test['SEXUAL'] == True) | (test['ABUSE'] == True)\n",
    "test_set = test[['text', 'immoral']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## badwords 가져오기\n",
    "https://github.com/organization/Gentleman/blob/master/resources/badwords.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# badwords, train\n",
    "with open(r\".\\data\\train\\labled\\badwords.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "badwords = pd.DataFrame(data['badwords'], columns= ['text'])\n",
    "badwords['immoral'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가져온 데이터 train/test 나눠서 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_sl = int(len(immoral) * 0.8)\n",
    "cl_sl = int(len(clean) * 0.8)\n",
    "cp_sl = int(len(champions) * 0.8)\n",
    "bw_sl = int(len(badwords) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((369610, 2), (46830, 2))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train_set, immoral[:im_sl], clean[:cl_sl], badwords[:bw_sl], champions[:cp_sl]])\n",
    "test = pd.concat([test_set, immoral[im_sl:], clean[cl_sl:], badwords[bw_sl:], champions[cp_sl:]])\n",
    "train['immoral'] = train['immoral'].apply(lambda x : 1 if x == True else 0)\n",
    "test['immoral'] = test['immoral'].apply(lambda x : 1 if x == True else 0)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((778841, 3), (247, 2))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.to_csv(r'data/train_final.csv', index = False)\n",
    "# test.to_csv(r'data/test_final.csv', index = False)\n",
    "# train = pd.read_csv(r'data/train_aug2.csv')\n",
    "# test = pd.read_csv(r'data/test_final.csv')\n",
    "# train = pd.read_csv(r'data/train_aug.csv')\n",
    "# test = pd.read_csv(r'data/test_aug.csv')\n",
    "\n",
    "test = pd.read_csv(r'data/champions_t.csv')\n",
    "train = train.dropna()\n",
    "test = test.dropna()\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 자모음으로 쪼개기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x : j2hcj(h2j(x)))\n",
    "test['text'] = test['text'].apply(lambda x : j2hcj(h2j(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swap rows\n",
    "train = train.sample(frac = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToList(text):\n",
    "    li = []\n",
    "    for t in text:\n",
    "        li.append(list(t.strip()))\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자모음 쪼갠 텍스트를 한글자씩 리스트로 만들어서 데이터셋 만든다\n",
    "def textToList(text):\n",
    "    li = []\n",
    "    for t in text:\n",
    "        li.append(list(t))\n",
    "    return li\n",
    "\n",
    "train_list = textToList(train['text'])\n",
    "train_label = train['immoral']\n",
    "\n",
    "test_list = textToList(test['text'])\n",
    "test_label = test['immoral']\n",
    "\n",
    "# X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 최대 길이 정해야되니까 문장 길이 분포가 얼마나 되어있는지 확인한다\n",
    "# li_len = []\n",
    "# for t in train_list:\n",
    "#     li_len.append(len(t))\n",
    "# import seaborn as sns\n",
    "# sns.set_theme(style=\"darkgrid\")\n",
    "# sns.histplot(li_len)\n",
    "## 100으로 자르면 문장 잘리는 아이들 비율이 2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 단어 개수:  166\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=110)\n",
    "tokenizer.fit_on_texts(train_list) # 단어 인덱스 구축\n",
    "text_sequences = tokenizer.texts_to_sequences(train_list) # 문자열 -> 인덱스 리스트\n",
    "                                                            # '나는 천재다 나는 멋있다' -> [1, 2, 1, 3]\n",
    "# train data로 fit 시켜준 tokenizer로 test data도 인덱스로 변경\n",
    "text_sequences_test = tokenizer.texts_to_sequences(test_list)\n",
    "\n",
    "word_vocab = tokenizer.word_index # 딕셔너리 형태\n",
    "print(\"전체 단어 개수: \", len(word_vocab)) # 전체 단어 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train input data tensor: (778841, 100)\n",
      "Shape of train label tensor: (778841,)\n",
      "Shape of test input data tensor: (247, 100)\n",
      "Shape of test label tensor: (247,)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100 # 문장 최대 길이\n",
    "\n",
    "X_train = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='pre') # 문장의 길이가 100 글자가 넘어가면 자르고, 모자르면 0으로 채워 넣는다.\n",
    "y_train = np.array(train_label) # 각 리뷰의 감정을 넘파이 배열로 만든다.\n",
    "\n",
    "X_test = pad_sequences(text_sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='pre') # 문장의 길이가 100 글자가 넘어가면 자르고, 모자르면 0으로 채워 넣는다.\n",
    "y_test = np.array(test_label) # 각 리뷰의 감정을 넘파이 배열로 만든다.\n",
    "\n",
    "print('Shape of train input data tensor:', X_train.shape) # 리뷰 데이터의 형태 확인\n",
    "print('Shape of train label tensor:', y_train.shape) # 감정 데이터 형태 확인\n",
    "print('Shape of test input data tensor:', X_test.shape) # 리뷰 데이터의 형태 확인\n",
    "print('Shape of test label tensor:', y_test.shape) # 감정 데이터 형태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          5344      \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 16)          2576      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, None, 16)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 16)          1296      \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 16)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,361\n",
      "Trainable params: 9,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Flatten, Dropout\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "embedding_dim = 50 # 임베딩 벡터의 차원\n",
    "dropout_ratio = 0.3 # 드롭아웃 비율\n",
    "num_filters = 256 # 커널의 수\n",
    "kernel_size = 3 # 커널의 크기\n",
    "hidden_units = 128 # 뉴런의 수\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(len(word_vocab)+1, embedding_dim))\n",
    "# model.add(Conv1D(256, kernel_size, padding='valid', activation='relu'))\n",
    "# model.add(Conv1D(256, kernel_size, padding='valid', activation='relu'))\n",
    "# model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(hidden_units, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "vocab_size = 30000\n",
    "word_vector_dim = 32\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(len(word_vocab)+1, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.Conv1D(16, 5, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 5, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'mean_squared_error', metrics=['accuracy',\n",
    "                                                                            tf.keras.metrics.TrueNegatives(name='true_negatives'),\n",
    "                                                                            tf.keras.metrics.TruePositives(name='true_positives'),\n",
    "                                                                            tf.keras.metrics.FalseNegatives(name='false_negatives'),\n",
    "                                                                            tf.keras.metrics.FalsePositives(name='false_positives')])\n",
    "# 이진 분류이므로 손실함수는 binary_crossentropy 사용, 에폭마다 정확도를 보기 위해 accuracy 적용\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6231/6231 [==============================] - 129s 21ms/step - loss: 0.1417 - accuracy: 0.8006 - true_negatives: 206050.0000 - true_positives: 292768.0000 - false_negatives: 63472.0000 - false_positives: 60782.0000 - val_loss: 0.1479 - val_accuracy: 0.7915 - val_true_negatives: 55895.0000 - val_true_positives: 67401.0000 - val_false_negatives: 21677.0000 - val_false_positives: 10796.0000 - lr: 4.0000e-05\n",
      "Epoch 2/100\n",
      "6231/6231 [==============================] - 127s 20ms/step - loss: 0.1416 - accuracy: 0.8008 - true_negatives: 206213.0000 - true_positives: 292733.0000 - false_negatives: 63507.0000 - false_positives: 60619.0000 - val_loss: 0.1495 - val_accuracy: 0.7888 - val_true_negatives: 56769.0000 - val_true_positives: 66095.0000 - val_false_negatives: 22983.0000 - val_false_positives: 9922.0000 - lr: 4.0000e-05\n",
      "Epoch 3/100\n",
      "6231/6231 [==============================] - 121s 19ms/step - loss: 0.1416 - accuracy: 0.8008 - true_negatives: 206393.0000 - true_positives: 292571.0000 - false_negatives: 63669.0000 - false_positives: 60439.0000 - val_loss: 0.1480 - val_accuracy: 0.7912 - val_true_negatives: 56037.0000 - val_true_positives: 67208.0000 - val_false_negatives: 21870.0000 - val_false_positives: 10654.0000 - lr: 4.0000e-05\n",
      "Epoch 4/100\n",
      "6231/6231 [==============================] - 90s 14ms/step - loss: 0.1412 - accuracy: 0.8014 - true_negatives: 206622.0000 - true_positives: 292690.0000 - false_negatives: 63550.0000 - false_positives: 60210.0000 - val_loss: 0.1479 - val_accuracy: 0.7916 - val_true_negatives: 56079.0000 - val_true_positives: 67234.0000 - val_false_negatives: 21844.0000 - val_false_positives: 10612.0000 - lr: 8.0000e-06\n",
      "Epoch 5/100\n",
      "6231/6231 [==============================] - 138s 22ms/step - loss: 0.1411 - accuracy: 0.8016 - true_negatives: 206662.0000 - true_positives: 292819.0000 - false_negatives: 63421.0000 - false_positives: 60170.0000 - val_loss: 0.1476 - val_accuracy: 0.7922 - val_true_negatives: 55907.0000 - val_true_positives: 67496.0000 - val_false_negatives: 21582.0000 - val_false_positives: 10784.0000 - lr: 8.0000e-06\n",
      "Epoch 6/100\n",
      "6231/6231 [==============================] - 113s 18ms/step - loss: 0.1410 - accuracy: 0.8020 - true_negatives: 206586.0000 - true_positives: 293125.0000 - false_negatives: 63115.0000 - false_positives: 60246.0000 - val_loss: 0.1485 - val_accuracy: 0.7903 - val_true_negatives: 56454.0000 - val_true_positives: 66651.0000 - val_false_negatives: 22427.0000 - val_false_positives: 10237.0000 - lr: 8.0000e-06\n",
      "Epoch 7/100\n",
      "6231/6231 [==============================] - 133s 21ms/step - loss: 0.1411 - accuracy: 0.8016 - true_negatives: 206488.0000 - true_positives: 292956.0000 - false_negatives: 63284.0000 - false_positives: 60344.0000 - val_loss: 0.1478 - val_accuracy: 0.7920 - val_true_negatives: 56080.0000 - val_true_positives: 67292.0000 - val_false_negatives: 21786.0000 - val_false_positives: 10611.0000 - lr: 8.0000e-06\n",
      "Epoch 8/100\n",
      "6231/6231 [==============================] - 113s 18ms/step - loss: 0.1410 - accuracy: 0.8018 - true_negatives: 206650.0000 - true_positives: 292955.0000 - false_negatives: 63285.0000 - false_positives: 60182.0000 - val_loss: 0.1479 - val_accuracy: 0.7916 - val_true_negatives: 56167.0000 - val_true_positives: 67143.0000 - val_false_negatives: 21935.0000 - val_false_positives: 10524.0000 - lr: 1.6000e-06\n",
      "Epoch 9/100\n",
      "6231/6231 [==============================] - 127s 20ms/step - loss: 0.1411 - accuracy: 0.8020 - true_negatives: 206666.0000 - true_positives: 293062.0000 - false_negatives: 63178.0000 - false_positives: 60166.0000 - val_loss: 0.1478 - val_accuracy: 0.7918 - val_true_negatives: 56108.0000 - val_true_positives: 67225.0000 - val_false_negatives: 21853.0000 - val_false_positives: 10583.0000 - lr: 1.6000e-06\n"
     ]
    }
   ],
   "source": [
    "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", \n",
    "    factor=0.2, # new_lr = lr * factor.\n",
    "    patience=2,\n",
    "    cooldown=2, # number of epochs to wait before resuming normal operation after lr has been reduced.  \n",
    "    min_lr=0\n",
    ")\n",
    "model.fit(X_train, y_train, epochs= 100, batch_size = 100, validation_split=0.2, callbacks=[early, reduce_lr])\n",
    "\n",
    "model.save('conv_red.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.8219 - true_negatives: 4.0000 - true_positives: 199.0000 - false_negatives: 36.0000 - false_positives: 8.0000  \n",
      "[0.1311338096857071, 0.8218623399734497, 4.0, 199.0, 36.0, 8.0]\n"
     ]
    }
   ],
   "source": [
    "pred_y_train = model.predict(X_train)\n",
    "print(model.evaluate(X_train, y_train))\n",
    "\n",
    "pred_y_test = model.predict(X_test)\n",
    "print(model.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_org = pd.read_csv(r'data/train_aug.csv')\n",
    "train_org = train_org.dropna()\n",
    "test_org = pd.read_csv(r'data/test_aug.csv')\n",
    "train_org['pred'] = np.where(pred_y_train > 0.5, 1, 0)\n",
    "test_org['pred'] = np.where(pred_y_test > 0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.67      0.78       688\n",
      "         1.0       0.47      0.85      0.60       235\n",
      "\n",
      "    accuracy                           0.72       923\n",
      "   macro avg       0.70      0.76      0.69       923\n",
      "weighted avg       0.81      0.72      0.74       923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test = pd.read_csv(r'data/champions_t.csv')\n",
    "test_org = pd.read_csv(r'data/champions_t.csv')\n",
    "test = test.fillna(0)\n",
    "test_org = test_org.fillna(0)\n",
    "test['text'] = test['text'].apply(lambda x : j2hcj(h2j(x)))\n",
    "test_list = textToList(test['text'])\n",
    "test_label = test['immoral']\n",
    "text_sequences_test = tokenizer.texts_to_sequences(test_list)\n",
    "X_test = pad_sequences(text_sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='pre') # 문장의 길이가 100 글자가 넘어가면 자르고, 모자르면 0으로 채워 넣는다.\n",
    "y_test = np.array(test_label) # 각 리뷰의 감정을 넘파이 배열로 만든다.\n",
    "\n",
    "pred_y_test = model2.predict(X_test)\n",
    "test_org['pred'] = np.where(pred_y_test > 0.5, 1, 0)\n",
    "print(classification_report(test_org['immoral'], test_org['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 875us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95       269\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.91       269\n",
      "   macro avg       0.50      0.46      0.48       269\n",
      "weighted avg       1.00      0.91      0.95       269\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\llljw\\AppData\\Local\\Temp\\ipykernel_17788\\2565546387.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  test = pd.read_csv(r\"C:\\Users\\llljw\\Downloads\\clean.txt\", sep = 'aaaaaaaaaaaaaaaaaaaaaaaa')\n",
      "c:\\Users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test = pd.read_csv(r\"C:\\Users\\llljw\\Downloads\\clean.txt\", sep = 'aaaaaaaaaaaaaaaaaaaaaaaa')\n",
    "test.columns = ['text']\n",
    "test['immoral'] = 0\n",
    "test_org = test.copy()\n",
    "test = test.fillna(0)\n",
    "test_org = test_org.fillna(0)\n",
    "test['text'] = test['text'].apply(lambda x : j2hcj(h2j(x)))\n",
    "test_list = textToList(test['text'])\n",
    "test_label = test['immoral']\n",
    "text_sequences_test = tokenizer.texts_to_sequences(test_list)\n",
    "X_test = pad_sequences(text_sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='pre') # 문장의 길이가 100 글자가 넘어가면 자르고, 모자르면 0으로 채워 넣는다.\n",
    "y_test = np.array(test_label) # 각 리뷰의 감정을 넘파이 배열로 만든다.\n",
    "\n",
    "pred_y_test = model2.predict(X_test)\n",
    "test_org['pred'] = np.where(pred_y_test > 0.5, 1, 0)\n",
    "print(classification_report(test_org['immoral'], test_org['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>immoral</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>캡쳐해서 인벤에 박제해야지</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>베인이 욕심부려서 못밀은거잖아</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>어디서 내가 욕심을 부렸는데?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>내가 핑을 쳤어 뭘했어?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미니언 먹는다고 암 말 안했는데</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>역시 팅ㅇ기신거였네요</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>화이팅</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>ㅎㅇㅎㅇ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>고소장ㄱㄱ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>내가고아인지 아닌지 어케알아요? ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>269 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text  immoral  pred\n",
       "0          캡쳐해서 인벤에 박제해야지        1     0\n",
       "1        베인이 욕심부려서 못밀은거잖아        1     0\n",
       "2        어디서 내가 욕심을 부렸는데?        1     0\n",
       "3           내가 핑을 쳤어 뭘했어?        1     0\n",
       "4       미니언 먹는다고 암 말 안했는데        1     0\n",
       "..                    ...      ...   ...\n",
       "264           역시 팅ㅇ기신거였네요        1     0\n",
       "265                   화이팅        1     0\n",
       "266                  ㅎㅇㅎㅇ        1     0\n",
       "267                 고소장ㄱㄱ        1     0\n",
       "268  내가고아인지 아닌지 어케알아요? ㅋㅋ        1     0\n",
       "\n",
       "[269 rows x 3 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>immoral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>사미라는 황밸인거같은데 무대는 리워크좀해라최소 치명타 50%고정정도만 해도 쓸만하지않음?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>드락 너프 씨게 먹었던데 이제 징수의 총 -&gt; 무한의 대검 가야 되나</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>사미라로 드락 별로라는 소리가 있는데 드락은 하위템이 롱소드 박박이라 가는거임 무대...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>서폿이 잘하면 무조건 이기는 챔프</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>드락 왜가는지 이해가 안됨;; 드락보다 무대가 훨씬 난데</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>클레드로 굶드라랑 월식 못 가게 너프먹는다니까 진짜 ㅈ같아서 겜 못해먹겠네 월식은 ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>진짜 개사기임아니 협곡말고 롤체 학살자 클레드요</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>진짜 태불방 롤백 시킬거 아니면 클레드는 건들면 안되지 ㅋㅋㅋ 선넘네 진짜</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>\"바텀에서 겨우 더블킬을 했다.\"\"나팔소리가 들렸다.\"\"여름이었다.\"(제압되었습니다!)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>ㄹㅇ 누군지도 몰랐다가 호되게 당함;;;</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>923 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  immoral\n",
       "0    사미라는 황밸인거같은데 무대는 리워크좀해라최소 치명타 50%고정정도만 해도 쓸만하지않음?      NaN\n",
       "1               드락 너프 씨게 먹었던데 이제 징수의 총 -> 무한의 대검 가야 되나      NaN\n",
       "2    사미라로 드락 별로라는 소리가 있는데 드락은 하위템이 롱소드 박박이라 가는거임 무대...      NaN\n",
       "3                                   서폿이 잘하면 무조건 이기는 챔프      NaN\n",
       "4                      드락 왜가는지 이해가 안됨;; 드락보다 무대가 훨씬 난데      NaN\n",
       "..                                                 ...      ...\n",
       "918  클레드로 굶드라랑 월식 못 가게 너프먹는다니까 진짜 ㅈ같아서 겜 못해먹겠네 월식은 ...      1.0\n",
       "919                         진짜 개사기임아니 협곡말고 롤체 학살자 클레드요      0.0\n",
       "920          진짜 태불방 롤백 시킬거 아니면 클레드는 건들면 안되지 ㅋㅋㅋ 선넘네 진짜      0.0\n",
       "921   \"바텀에서 겨우 더블킬을 했다.\"\"나팔소리가 들렸다.\"\"여름이었다.\"(제압되었습니다!)      0.0\n",
       "922                             ㄹㅇ 누군지도 몰랐다가 호되게 당함;;;      0.0\n",
       "\n",
       "[923 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.79      0.77    333523\n",
      "           1       0.24      0.21      0.23    108258\n",
      "\n",
      "    accuracy                           0.65    441781\n",
      "   macro avg       0.50      0.50      0.50    441781\n",
      "weighted avg       0.63      0.65      0.64    441781\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91     41924\n",
      "           1       0.81      0.65      0.72     14718\n",
      "\n",
      "    accuracy                           0.87     56642\n",
      "   macro avg       0.84      0.80      0.82     56642\n",
      "weighted avg       0.86      0.87      0.86     56642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(train_org['immoral'], train_org['pred']))\n",
    "print(classification_report(test_org['immoral'], test_org['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1464/1464 [==============================] - 8s 6ms/step\n",
      "1464/1464 [==============================] - 8s 5ms/step - loss: 1.2017 - accuracy: 0.4003\n",
      "[1.2017261981964111, 0.400341659784317]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.36      0.52     41924\n",
      "           1       0.12      0.78      0.21      4906\n",
      "\n",
      "    accuracy                           0.40     46830\n",
      "   macro avg       0.53      0.57      0.36     46830\n",
      "weighted avg       0.85      0.40      0.48     46830\n",
      "\n",
      "******************************\n",
      "1464/1464 [==============================] - 1s 938us/step\n",
      "1464/1464 [==============================] - 2s 1ms/step - loss: 0.1350 - accuracy: 0.8215 - true_negatives: 35158.0000 - true_positives: 3312.0000 - false_negatives: 1594.0000 - false_positives: 6766.0000\n",
      "[0.1350131332874298, 0.8214819431304932, 35158.0, 3312.0, 1594.0, 6766.0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.84      0.89     41924\n",
      "           1       0.33      0.68      0.44      4906\n",
      "\n",
      "    accuracy                           0.82     46830\n",
      "   macro avg       0.64      0.76      0.67     46830\n",
      "weighted avg       0.89      0.82      0.85     46830\n",
      "\n",
      "******************************\n",
      "1464/1464 [==============================] - 11s 8ms/step\n",
      "1464/1464 [==============================] - 12s 8ms/step - loss: 0.6950 - accuracy: 0.1439\n",
      "[0.6950210332870483, 0.1439034789800644]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.05      0.09     41924\n",
      "           1       0.11      0.96      0.19      4906\n",
      "\n",
      "    accuracy                           0.14     46830\n",
      "   macro avg       0.51      0.50      0.14     46830\n",
      "weighted avg       0.82      0.14      0.10     46830\n",
      "\n",
      "******************************\n",
      "1464/1464 [==============================] - 12s 8ms/step\n",
      "1464/1464 [==============================] - 13s 9ms/step - loss: 0.4874 - accuracy: 0.4224 - true_negatives: 16042.0000 - true_positives: 3739.0000 - false_negatives: 1167.0000 - false_positives: 25882.0000\n",
      "[0.48744022846221924, 0.42240017652511597, 16042.0, 3739.0, 1167.0, 25882.0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.38      0.54     41924\n",
      "           1       0.13      0.76      0.22      4906\n",
      "\n",
      "    accuracy                           0.42     46830\n",
      "   macro avg       0.53      0.57      0.38     46830\n",
      "weighted avg       0.85      0.42      0.51     46830\n",
      "\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "model1 = tf.keras.models.load_model('cnn.h5')\n",
    "model2 = tf.keras.models.load_model('conv_red.h5')\n",
    "model3 = tf.keras.models.load_model('conv2.h5')\n",
    "model4 = tf.keras.models.load_model('conv3.h5')\n",
    "\n",
    "pred = []\n",
    "models = [model1, model2, model3, model4]\n",
    "for m in models:\n",
    "    pred_y_test = m.predict(X_test)\n",
    "    pred.append(pred_y_test.reshape(-1,))\n",
    "    print(m.evaluate(X_test, y_test))\n",
    "    pred_y_test = np.where(pred_y_test > 0.5, 1, 0)\n",
    "    print(classification_report(y_test, pred_y_test.reshape(-1,)))\n",
    "    print('*' * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0] = pred[0] * 0.4\n",
    "pred[1] = pred[1] * 0.82\n",
    "pred[2] = pred[2] * 0.14\n",
    "pred[3] = pred[3] * 0.42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.61      0.75     41924\n",
      "           1       0.19      0.80      0.31      4906\n",
      "\n",
      "    accuracy                           0.63     46830\n",
      "   macro avg       0.58      0.70      0.53     46830\n",
      "weighted avg       0.88      0.63      0.70     46830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(pred)\n",
    "mn = df.sum(axis = 0).values\n",
    "mn = np.where(mn > 0.9, 1, 0)\n",
    "print(classification_report(y_test, mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lime\n",
      "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
      "     ---------------------------------------- 0.0/275.7 kB ? eta -:--:--\n",
      "     ------------------------------------- 275.7/275.7 kB 16.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: matplotlib in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from lime) (3.7.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from lime) (1.25.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from lime) (1.11.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from lime) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from lime) (1.3.0)\n",
      "Collecting scikit-image>=0.12 (from lime)\n",
      "  Obtaining dependency information for scikit-image>=0.12 from https://files.pythonhosted.org/packages/f3/93/65601f7577d6fd49ec23bf8fb58c04d8170b06a1544452ae2ea9f59bf11f/scikit_image-0.21.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading scikit_image-0.21.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from scikit-image>=0.12->lime) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from scikit-image>=0.12->lime) (10.0.0)\n",
      "Collecting imageio>=2.27 (from scikit-image>=0.12->lime)\n",
      "  Obtaining dependency information for imageio>=2.27 from https://files.pythonhosted.org/packages/eb/21/662994d78d8623055f8ffa91838e28f04b2a34bd5d8d6dbc6c7573285ed6/imageio-2.31.3-py3-none-any.whl.metadata\n",
      "  Downloading imageio-2.31.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image>=0.12->lime)\n",
      "  Obtaining dependency information for tifffile>=2022.8.12 from https://files.pythonhosted.org/packages/12/3e/89513f44a10c625121b7d5bc54390d7ac7f2c92a19755c052888febf9730/tifffile-2023.8.30-py3-none-any.whl.metadata\n",
      "  Downloading tifffile-2023.8.30-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting PyWavelets>=1.1.1 (from scikit-image>=0.12->lime)\n",
      "  Downloading PyWavelets-1.4.1-cp310-cp310-win_amd64.whl (4.2 MB)\n",
      "     ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "     --------------------------- ------------ 2.9/4.2 MB 60.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.2/4.2 MB 53.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from scikit-image>=0.12->lime) (23.1)\n",
      "Collecting lazy_loader>=0.2 (from scikit-image>=0.12->lime)\n",
      "  Obtaining dependency information for lazy_loader>=0.2 from https://files.pythonhosted.org/packages/a1/c3/65b3814e155836acacf720e5be3b5757130346670ac454fee29d3eda1381/lazy_loader-0.3-py3-none-any.whl.metadata\n",
      "  Downloading lazy_loader-0.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from scikit-learn>=0.18->lime) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from scikit-learn>=0.18->lime) (3.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from matplotlib->lime) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from matplotlib->lime) (4.41.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from matplotlib->lime) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from matplotlib->lime) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from matplotlib->lime) (2.8.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from tqdm->lime) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
      "Downloading scikit_image-0.21.0-cp310-cp310-win_amd64.whl (22.8 MB)\n",
      "   ---------------------------------------- 0.0/22.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.1/22.8 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 6.5/22.8 MB 82.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 10.4/22.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 15.7/22.8 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 21.1/22.8 MB 129.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  22.8/22.8 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 22.8/22.8 MB 72.5 MB/s eta 0:00:00\n",
      "Downloading imageio-2.31.3-py3-none-any.whl (313 kB)\n",
      "   ---------------------------------------- 0.0/313.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 313.0/313.0 kB ? eta 0:00:00\n",
      "Downloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
      "Downloading tifffile-2023.8.30-py3-none-any.whl (221 kB)\n",
      "   ---------------------------------------- 0.0/221.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 221.9/221.9 kB 13.2 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: lime\n",
      "  Building wheel for lime (setup.py): started\n",
      "  Building wheel for lime (setup.py): finished with status 'done'\n",
      "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283846 sha256=8cd81ce4a0ea4a5c117258f06cb5bd1a3f8a05dd1d1810d58ae506af13403e16\n",
      "  Stored in directory: c:\\users\\llljw\\appdata\\local\\pip\\cache\\wheels\\fd\\a2\\af\\9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n",
      "Successfully built lime\n",
      "Installing collected packages: tifffile, PyWavelets, lazy_loader, imageio, scikit-image, lime\n",
      "Successfully installed PyWavelets-1.4.1 imageio-2.31.3 lazy_loader-0.3 lime-0.2.0.1 scikit-image-0.21.0 tifffile-2023.8.30\n"
     ]
    }
   ],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Prediction :  [[0.09567054]]\n",
      "Actual :      0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\code\\skkukdt_minzy\\DeepLearning\\project.ipynb Cell 42\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/skkukdt_minzy/DeepLearning/project.ipynb#X61sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPrediction : \u001b[39m\u001b[39m\"\u001b[39m, model2\u001b[39m.\u001b[39mpredict(X_test[idx]\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/skkukdt_minzy/DeepLearning/project.ipynb#X61sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mActual :     \u001b[39m\u001b[39m\"\u001b[39m, y_test[idx])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/code/skkukdt_minzy/DeepLearning/project.ipynb#X61sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m explanation \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39mexplain_instance(X_test[idx], model2\u001b[39m.\u001b[39;49mpredict_proba, labels\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m,) ,num_samples\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "from lime import lime_tabular\n",
    "\n",
    "explainer = lime_tabular.LimeTabularExplainer(X_train, mode=\"classification\")\n",
    "\n",
    "idx = 3\n",
    "\n",
    "print(\"Prediction : \", model2.predict(X_test[idx].reshape(1,-1)))\n",
    "print(\"Actual :     \", y_test[idx])\n",
    "\n",
    "explanation = explainer.explain_instance(X_test[idx], model2, labels=(0,) ,num_samples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\code\\skkukdt_minzy\\DeepLearning\\project.ipynb Cell 43\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/skkukdt_minzy/DeepLearning/project.ipynb#X66sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pipe\u001b[39m=\u001b[39mmake_pipeline(model2) \u001b[39m# 벡터화 + 나이브베이즈 모델\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/skkukdt_minzy/DeepLearning/project.ipynb#X66sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# 파이프라인을 이용해 데이터 하나에 대한 (벡터화+예측확률)을 반환하자. (fit, transform 과정이 불필요)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/code/skkukdt_minzy/DeepLearning/project.ipynb#X66sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m predict_classes\u001b[39m=\u001b[39mpipe\u001b[39m.\u001b[39;49mpredict_proba([x_test[\u001b[39m0\u001b[39m]])\u001b[39m.\u001b[39mround(\u001b[39m3\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/skkukdt_minzy/DeepLearning/project.ipynb#X66sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(predict_classes)\n",
      "File \u001b[1;32mc:\\Users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages\\sklearn\\utils\\_available_if.py:31\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__\u001b[1;34m(self, obj, owner)\u001b[0m\n\u001b[0;32m     25\u001b[0m attr_err \u001b[39m=\u001b[39m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m     26\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(owner\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattribute_name)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[39m# delegate only on instances, not the classes.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[39m# this is to allow access to the docstrings.\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck(obj):\n\u001b[0;32m     32\u001b[0m         \u001b[39mraise\u001b[39;00m attr_err\n\u001b[0;32m     33\u001b[0m     out \u001b[39m=\u001b[39m MethodType(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn, obj)\n",
      "File \u001b[1;32mc:\\Users\\llljw\\anaconda3\\envs\\mzpark\\lib\\site-packages\\sklearn\\pipeline.py:44\u001b[0m, in \u001b[0;36m_final_estimator_has.<locals>.check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     43\u001b[0m     \u001b[39m# raise original `AttributeError` if `attr` does not exist\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator, attr)\n\u001b[0;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipe=make_pipeline(model2) # 벡터화 + 나이브베이즈 모델\n",
    "\n",
    "# 파이프라인을 이용해 데이터 하나에 대한 (벡터화+예측확률)을 반환하자. (fit, transform 과정이 불필요)\n",
    "predict_classes=pipe.predict_proba([x_test[0]]).round(3)[0]\n",
    "\n",
    "print(predict_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer=LimeTextExplainer(class_names=[0, 1])\n",
    "exp=explainer.explain_instance(X_test.data[0], # 첫 번째 텍스트 데이터.\n",
    "                              pipe.predict_proba,# 텍스트를 입력으로 받고,(벡터화를 거쳐) 카테고리 별 확률을 반환한다.\n",
    "                              top_labels=1) # 가장 확률이 높은 클래스만 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=newsgroups_test.data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mzp",
   "language": "python",
   "name": "mzpark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
