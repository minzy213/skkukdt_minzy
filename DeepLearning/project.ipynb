{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt # komoran, han, kkma\n",
    "from jamo import h2j, j2hcj\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 가져오기, 학습 데이터셋으로 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인벤에서 수집한 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10380\\2151298477.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  immoral = pd.read_csv(r'data\\immoral.txt', sep = 'asssssssss', header = None)\n"
     ]
    }
   ],
   "source": [
    "immoral = pd.read_csv(r'data\\immoral.txt', sep = 'asssssssss', header = None)\n",
    "immoral.columns = ['text']\n",
    "immoral['immoral'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10380\\2827705178.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  clean = pd.read_csv(r'data\\clean.txt', sep = 'asssssssss', header = None)\n"
     ]
    }
   ],
   "source": [
    "clean = pd.read_csv(r'data\\clean.txt', sep = 'asssssssss', header = None)\n",
    "clean.columns = ['text']\n",
    "clean['immoral'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## op.gg 데이터 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lii = []\n",
    "headers = {\n",
    "    'Referer':'https://www.op.gg/',\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\n",
    "}\n",
    "for chm in champ:\n",
    "    n = 1\n",
    "    while 1:\n",
    "        url = f'https://op.gg/api/v1.0/internal/bypass/champions/{chm}/comments?&sort=popular&page={n}&limit=100&hl=ko_KR&is_latest_version=false'\n",
    "        n += 1\n",
    "        resp = requests.get(url, headers = headers)\n",
    "        dict_ch = json.loads(resp.text)\n",
    "        if len(dict_ch['data']) == 0:\n",
    "            break\n",
    "        \n",
    "        for dat in dict_ch['data']:\n",
    "            lii.append(dat['content'].replace('\\n', ''))\n",
    "        time.sleep(5)\n",
    "    \n",
    "    print(chm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>immoral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>얘 와맆에 곰돌이가 에어본 시키는 거 생겼던데 여기에도 생기나</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>자기고향 찾아오네</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>몇달만의 고향이냐</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>점멸 있을 때 한정 1티어</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>스킬 사거리 표시 없이 하면 암걸리는 챔</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7968</th>\n",
       "      <td>의외로 꼴짤이 별로 없음</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7969</th>\n",
       "      <td>나피리 근데 초반 라인전도 약하고 카운터도 많음.그리고 돌진기가 다른 암살자와 다르...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7970</th>\n",
       "      <td>0/6/0 야스오:야이 개쌔꺄 이게 챔이냐 야발4/0/2 나피리:개 맞는데0/8/0...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7971</th>\n",
       "      <td>얘 한테 지면 사람아닌래끼다버프좀 하고 성능좀 버프히라못 쓸 정도다</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7972</th>\n",
       "      <td>버프좀 해줘</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7973 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text immoral\n",
       "0                    얘 와맆에 곰돌이가 에어본 시키는 거 생겼던데 여기에도 생기나        \n",
       "1                                             자기고향 찾아오네        \n",
       "2                                             몇달만의 고향이냐        \n",
       "3                                        점멸 있을 때 한정 1티어        \n",
       "4                                스킬 사거리 표시 없이 하면 암걸리는 챔        \n",
       "...                                                 ...     ...\n",
       "7968                                      의외로 꼴짤이 별로 없음        \n",
       "7969  나피리 근데 초반 라인전도 약하고 카운터도 많음.그리고 돌진기가 다른 암살자와 다르...        \n",
       "7970  0/6/0 야스오:야이 개쌔꺄 이게 챔이냐 야발4/0/2 나피리:개 맞는데0/8/0...        \n",
       "7971              얘 한테 지면 사람아닌래끼다버프좀 하고 성능좀 버프히라못 쓸 정도다        \n",
       "7972                                             버프좀 해줘        \n",
       "\n",
       "[7973 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lii = pd.DataFrame(lii, columns = ['text'])\n",
    "df_lii['immoral'] = ''\n",
    "df_lii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### op.gg 데이터 직접 라벨링, 라벨링 된 csv 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lii.to_csv(r'data/champions.csv', index = False)\n",
    "df_lii = pd.read_csv(r'data/champions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10380\\2204739627.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'False' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  champions.loc[champions['immoral'] == 0, 'immoral'] = False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>immoral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>얘 와맆에 곰돌이가 에어본 시키는 거 생겼던데 여기에도 생기나</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>자기고향 찾아오네</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>몇달만의 고향이냐</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>점멸 있을 때 한정 1티어</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>스킬 사거리 표시 없이 하면 암걸리는 챔</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6458</th>\n",
       "      <td>클레드로 굶드라랑 월식 못 가게 너프먹는다니까 진짜 ㅈ같아서 겜 못해먹겠네 월식은 ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6459</th>\n",
       "      <td>진짜 개사기임아니 협곡말고 롤체 학살자 클레드요</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6460</th>\n",
       "      <td>진짜 태불방 롤백 시킬거 아니면 클레드는 건들면 안되지 ㅋㅋㅋ 선넘네 진짜</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6461</th>\n",
       "      <td>\"바텀에서 겨우 더블킬을 했다.\"\"나팔소리가 들렸다.\"\"여름이었다.\"(제압되었습니다!)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6462</th>\n",
       "      <td>ㄹㅇ 누군지도 몰랐다가 호되게 당함;;;</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6463 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text immoral\n",
       "0                    얘 와맆에 곰돌이가 에어본 시키는 거 생겼던데 여기에도 생기나   False\n",
       "1                                             자기고향 찾아오네   False\n",
       "2                                             몇달만의 고향이냐   False\n",
       "3                                        점멸 있을 때 한정 1티어   False\n",
       "4                                스킬 사거리 표시 없이 하면 암걸리는 챔   False\n",
       "...                                                 ...     ...\n",
       "6458  클레드로 굶드라랑 월식 못 가게 너프먹는다니까 진짜 ㅈ같아서 겜 못해먹겠네 월식은 ...    True\n",
       "6459                         진짜 개사기임아니 협곡말고 롤체 학살자 클레드요   False\n",
       "6460          진짜 태불방 롤백 시킬거 아니면 클레드는 건들면 안되지 ㅋㅋㅋ 선넘네 진짜   False\n",
       "6461   \"바텀에서 겨우 더블킬을 했다.\"\"나팔소리가 들렸다.\"\"여름이었다.\"(제압되었습니다!)   False\n",
       "6462                             ㄹㅇ 누군지도 몰랐다가 호되게 당함;;;   False\n",
       "\n",
       "[6463 rows x 2 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "champions = pd.read_csv(r'data\\champions.csv')\n",
    "champions.columns = ['text','immoral']\n",
    "champions.loc[champions['immoral'] == 0, 'immoral'] = False\n",
    "champions.loc[champions['immoral'] == 1, 'immoral'] = True\n",
    "champions = champions.fillna(False)\n",
    "champions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ai hub 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai hub train set\n",
    "li = []\n",
    "for i in range(1, 6):\n",
    "    with open(r'.\\data\\train\\labled\\talksets-train-' + str(i) + '.json') as f:\n",
    "        data = json.load(f)\n",
    "    for d in data:\n",
    "        for s in d['sentences']:\n",
    "            li.append([s['origin_text'], s['is_immoral'], s['intensity'], \n",
    "                    'CENSURE' in s['types'], 'HATE' in s['types'], 'DISCRIMINATION' in s['types'], \n",
    "                    'SEXUAL' in s['types'], 'ABUSE' in s['types'], 'VIOLENCE' in s['types'], 'CRIME' in s['types']])\n",
    "            \n",
    "train = pd.DataFrame(li, columns=['text', 'immoral', 'intensity', 'CENSURE', 'HATE', 'DISCRIMINATION', 'SEXUAL', 'ABUSE', 'VIOLENCE', 'CRIME'])\n",
    "train['text'] = train['text'].apply(lambda x : re.sub('#@.+?#', '', x.replace('#@#', '')))\n",
    "\n",
    "train['immoral'] = (train['SEXUAL'] == True) | (train['ABUSE'] == True)\n",
    "train_set = train[['text', 'immoral']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai hub test set\n",
    "li = []\n",
    "with open(r'.\\data\\valid\\labled\\talksets-train-6.json') as f:\n",
    "    data = json.load(f)\n",
    "for d in data:\n",
    "    for s in d['sentences']:\n",
    "        li.append([s['origin_text'], s['is_immoral'], s['intensity'], \n",
    "                'CENSURE' in s['types'], 'HATE' in s['types'], 'DISCRIMINATION' in s['types'], \n",
    "                'SEXUAL' in s['types'], 'ABUSE' in s['types'], 'VIOLENCE' in s['types'], 'CRIME' in s['types']])\n",
    "test = pd.DataFrame(li, columns=['text', 'immoral', 'intensity', 'CENSURE', 'HATE', 'DISCRIMINATION', 'SEXUAL', 'ABUSE', 'VIOLENCE', 'CRIME'])\n",
    "\n",
    "test['text'] = test['text'].apply(lambda x : re.sub('#@.+?#', '', x.replace('#@#', '')))\n",
    "\n",
    "test['immoral'] = (test['SEXUAL'] == True) | (test['ABUSE'] == True)\n",
    "test_set = test[['text', 'immoral']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## badwords 가져오기\n",
    "https://github.com/organization/Gentleman/blob/master/resources/badwords.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# badwords, train\n",
    "with open(r\".\\data\\train\\labled\\badwords.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "badwords = pd.DataFrame(data['badwords'], columns= ['text'])\n",
    "badwords['immoral'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가져온 데이터 train/test 나눠서 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_sl = int(len(immoral) * 0.8)\n",
    "cl_sl = int(len(clean) * 0.8)\n",
    "cp_sl = int(len(champions) * 0.8)\n",
    "bw_sl = int(len(badwords) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((369610, 2), (46830, 2))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train_set, immoral[:im_sl], clean[:cl_sl], badwords[:bw_sl], champions[:cp_sl]])\n",
    "test = pd.concat([test_set, immoral[im_sl:], clean[cl_sl:], badwords[bw_sl:], champions[cp_sl:]])\n",
    "train['immoral'] = train['immoral'].apply(lambda x : 1 if x == True else 0)\n",
    "test['immoral'] = test['immoral'].apply(lambda x : 1 if x == True else 0)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((441781, 2), (56642, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.to_csv(r'data/train_final.csv', index = False)\n",
    "# test.to_csv(r'data/test_final.csv', index = False)\n",
    "# train = pd.read_csv(r'data/train_final.csv')\n",
    "# test = pd.read_csv(r'data/test_final.csv')\n",
    "train = pd.read_csv(r'data/train_aug.csv')\n",
    "test = pd.read_csv(r'data/test_aug.csv')\n",
    "\n",
    "train = train.dropna()\n",
    "test = test.dropna()\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 자모음으로 쪼개기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x : j2hcj(h2j(x)))\n",
    "test['text'] = test['text'].apply(lambda x : j2hcj(h2j(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swap rows\n",
    "train = train.sample(frac = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToList(text):\n",
    "    li = []\n",
    "    for t in text:\n",
    "        li.append(list(t.strip()))\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자모음 쪼갠 텍스트를 한글자씩 리스트로 만들어서 데이터셋 만든다\n",
    "def textToList(text):\n",
    "    li = []\n",
    "    for t in text:\n",
    "        li.append(list(t))\n",
    "    return li\n",
    "\n",
    "train_list = textToList(train['text'])\n",
    "train_label = train['immoral']\n",
    "\n",
    "test_list = textToList(test['text'])\n",
    "test_label = test['immoral']\n",
    "\n",
    "# X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 최대 길이 정해야되니까 문장 길이 분포가 얼마나 되어있는지 확인한다\n",
    "# li_len = []\n",
    "# for t in train_list:\n",
    "#     li_len.append(len(t))\n",
    "# import seaborn as sns\n",
    "# sns.set_theme(style=\"darkgrid\")\n",
    "# sns.histplot(li_len)\n",
    "## 100으로 자르면 문장 잘리는 아이들 비율이 2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 단어 개수:  166\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=110)\n",
    "tokenizer.fit_on_texts(train_list) # 단어 인덱스 구축\n",
    "text_sequences = tokenizer.texts_to_sequences(train_list) # 문자열 -> 인덱스 리스트\n",
    "                                                            # '나는 천재다 나는 멋있다' -> [1, 2, 1, 3]\n",
    "# train data로 fit 시켜준 tokenizer로 test data도 인덱스로 변경\n",
    "text_sequences_test = tokenizer.texts_to_sequences(test_list)\n",
    "\n",
    "word_vocab = tokenizer.word_index # 딕셔너리 형태\n",
    "print(\"전체 단어 개수: \", len(word_vocab)) # 전체 단어 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train input data tensor: (441781, 100)\n",
      "Shape of train label tensor: (441781,)\n",
      "Shape of test input data tensor: (56642, 100)\n",
      "Shape of test label tensor: (56642,)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100 # 문장 최대 길이\n",
    "\n",
    "X_train = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='pre') # 문장의 길이가 100 글자가 넘어가면 자르고, 모자르면 0으로 채워 넣는다.\n",
    "y_train = np.array(train_label) # 각 리뷰의 감정을 넘파이 배열로 만든다.\n",
    "\n",
    "X_test = pad_sequences(text_sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='pre') # 문장의 길이가 100 글자가 넘어가면 자르고, 모자르면 0으로 채워 넣는다.\n",
    "y_test = np.array(test_label) # 각 리뷰의 감정을 넘파이 배열로 만든다.\n",
    "\n",
    "print('Shape of train input data tensor:', X_train.shape) # 리뷰 데이터의 형태 확인\n",
    "print('Shape of train label tensor:', y_train.shape) # 감정 데이터 형태 확인\n",
    "print('Shape of test input data tensor:', X_test.shape) # 리뷰 데이터의 형태 확인\n",
    "print('Shape of test label tensor:', y_test.shape) # 감정 데이터 형태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 50)          8350      \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 256)         38656     \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 256)         196864    \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, None, 256)         196864    \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 256)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 473,759\n",
      "Trainable params: 473,759\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Flatten, Dropout\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "embedding_dim = 50 # 임베딩 벡터의 차원\n",
    "dropout_ratio = 0.3 # 드롭아웃 비율\n",
    "num_filters = 256 # 커널의 수\n",
    "kernel_size = 3 # 커널의 크기\n",
    "hidden_units = 128 # 뉴런의 수\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_vocab)+1, embedding_dim))\n",
    "model.add(Conv1D(256, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(Conv1D(256, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_units, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'mean_squared_error', metrics=['accuracy',\n",
    "                                                                            tf.keras.metrics.TrueNegatives(name='true_negatives'),\n",
    "                                                                            tf.keras.metrics.TruePositives(name='true_positives'),\n",
    "                                                                            tf.keras.metrics.FalseNegatives(name='false_negatives'),\n",
    "                                                                            tf.keras.metrics.FalsePositives(name='false_positives')])\n",
    "# 이진 분류이므로 손실함수는 binary_crossentropy 사용, 에폭마다 정확도를 보기 위해 accuracy 적용\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3535/3535 [==============================] - 455s 128ms/step - loss: 0.1408 - accuracy: 0.8112 - true_negatives: 256119.0000 - true_positives: 30595.0000 - false_negatives: 56170.0000 - false_positives: 10540.0000 - val_loss: 0.1163 - val_accuracy: 0.8449 - val_true_negatives: 64631.0000 - val_true_positives: 10024.0000 - val_false_negatives: 11469.0000 - val_false_positives: 2233.0000 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "3535/3535 [==============================] - 439s 124ms/step - loss: 0.1132 - accuracy: 0.8517 - true_negatives: 254156.0000 - true_positives: 46871.0000 - false_negatives: 39894.0000 - false_positives: 12503.0000 - val_loss: 0.1093 - val_accuracy: 0.8566 - val_true_negatives: 63663.0000 - val_true_positives: 12023.0000 - val_false_negatives: 9470.0000 - val_false_positives: 3201.0000 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "3535/3535 [==============================] - 434s 123ms/step - loss: 0.1074 - accuracy: 0.8604 - true_negatives: 254561.0000 - true_positives: 49527.0000 - false_negatives: 37238.0000 - false_positives: 12098.0000 - val_loss: 0.1097 - val_accuracy: 0.8558 - val_true_negatives: 65095.0000 - val_true_positives: 10519.0000 - val_false_negatives: 10974.0000 - val_false_positives: 1769.0000 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "3535/3535 [==============================] - 449s 127ms/step - loss: 0.1044 - accuracy: 0.8643 - true_negatives: 254541.0000 - true_positives: 50907.0000 - false_negatives: 35858.0000 - false_positives: 12118.0000 - val_loss: 0.1104 - val_accuracy: 0.8537 - val_true_negatives: 65447.0000 - val_true_positives: 9983.0000 - val_false_negatives: 11510.0000 - val_false_positives: 1417.0000 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "3535/3535 [==============================] - 450s 127ms/step - loss: 0.0912 - accuracy: 0.8821 - true_negatives: 255256.0000 - true_positives: 56498.0000 - false_negatives: 30267.0000 - false_positives: 11403.0000 - val_loss: 0.0965 - val_accuracy: 0.8742 - val_true_negatives: 64405.0000 - val_true_positives: 12837.0000 - val_false_negatives: 8656.0000 - val_false_positives: 2459.0000 - lr: 2.0000e-04\n",
      "Epoch 6/100\n",
      "3535/3535 [==============================] - 488s 138ms/step - loss: 0.0871 - accuracy: 0.8884 - true_negatives: 255231.0000 - true_positives: 58735.0000 - false_negatives: 28030.0000 - false_positives: 11428.0000 - val_loss: 0.0960 - val_accuracy: 0.8765 - val_true_negatives: 62625.0000 - val_true_positives: 14819.0000 - val_false_negatives: 6674.0000 - val_false_positives: 4239.0000 - lr: 2.0000e-04\n",
      "Epoch 7/100\n",
      "3535/3535 [==============================] - 402s 114ms/step - loss: 0.0846 - accuracy: 0.8923 - true_negatives: 255273.0000 - true_positives: 60075.0000 - false_negatives: 26690.0000 - false_positives: 11386.0000 - val_loss: 0.0954 - val_accuracy: 0.8784 - val_true_negatives: 64119.0000 - val_true_positives: 13497.0000 - val_false_negatives: 7996.0000 - val_false_positives: 2745.0000 - lr: 2.0000e-04\n",
      "Epoch 8/100\n",
      "3535/3535 [==============================] - 422s 119ms/step - loss: 0.0825 - accuracy: 0.8953 - true_negatives: 255477.0000 - true_positives: 60930.0000 - false_negatives: 25835.0000 - false_positives: 11182.0000 - val_loss: 0.1056 - val_accuracy: 0.8670 - val_true_negatives: 65515.0000 - val_true_positives: 11094.0000 - val_false_negatives: 10399.0000 - val_false_positives: 1349.0000 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "3535/3535 [==============================] - 404s 114ms/step - loss: 0.0810 - accuracy: 0.8975 - true_negatives: 255568.0000 - true_positives: 61623.0000 - false_negatives: 25142.0000 - false_positives: 11091.0000 - val_loss: 0.1054 - val_accuracy: 0.8630 - val_true_negatives: 59556.0000 - val_true_positives: 16692.0000 - val_false_negatives: 4801.0000 - val_false_positives: 7308.0000 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "3535/3535 [==============================] - 445s 126ms/step - loss: 0.0741 - accuracy: 0.9080 - true_negatives: 256427.0000 - true_positives: 64473.0000 - false_negatives: 22292.0000 - false_positives: 10232.0000 - val_loss: 0.0915 - val_accuracy: 0.8838 - val_true_negatives: 63364.0000 - val_true_positives: 14723.0000 - val_false_negatives: 6770.0000 - val_false_positives: 3500.0000 - lr: 4.0000e-05\n",
      "Epoch 11/100\n",
      "3535/3535 [==============================] - 436s 123ms/step - loss: 0.0721 - accuracy: 0.9106 - true_negatives: 256692.0000 - true_positives: 65141.0000 - false_negatives: 21624.0000 - false_positives: 9967.0000 - val_loss: 0.0917 - val_accuracy: 0.8838 - val_true_negatives: 63142.0000 - val_true_positives: 14948.0000 - val_false_negatives: 6545.0000 - val_false_positives: 3722.0000 - lr: 4.0000e-05\n",
      "Epoch 12/100\n",
      "3535/3535 [==============================] - 423s 120ms/step - loss: 0.0711 - accuracy: 0.9126 - true_negatives: 256881.0000 - true_positives: 65648.0000 - false_negatives: 21117.0000 - false_positives: 9778.0000 - val_loss: 0.0917 - val_accuracy: 0.8846 - val_true_negatives: 63412.0000 - val_true_positives: 14747.0000 - val_false_negatives: 6746.0000 - val_false_positives: 3452.0000 - lr: 4.0000e-05\n",
      "Epoch 13/100\n",
      "3535/3535 [==============================] - 419s 118ms/step - loss: 0.0691 - accuracy: 0.9158 - true_negatives: 257292.0000 - true_positives: 66379.0000 - false_negatives: 20386.0000 - false_positives: 9367.0000 - val_loss: 0.0916 - val_accuracy: 0.8849 - val_true_negatives: 63292.0000 - val_true_positives: 14894.0000 - val_false_negatives: 6599.0000 - val_false_positives: 3572.0000 - lr: 8.0000e-06\n"
     ]
    }
   ],
   "source": [
    "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", \n",
    "    factor=0.2, # new_lr = lr * factor.\n",
    "    patience=2,\n",
    "    cooldown=2, # number of epochs to wait before resuming normal operation after lr has been reduced.  \n",
    "    min_lr=0\n",
    ")\n",
    "model.fit(X_train, y_train, epochs= 100, batch_size = 100, validation_split=0.2, callbacks=[early, reduce_lr])\n",
    "\n",
    "model.save('conv_red.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13806/13806 [==============================] - 177s 13ms/step\n",
      "13806/13806 [==============================] - 186s 13ms/step - loss: 0.0722 - accuracy: 0.9116 - true_negatives: 321063.0000 - true_positives: 81646.0000 - false_negatives: 26612.0000 - false_positives: 12460.0000\n",
      "[0.07222520560026169, 0.9115579724311829, 321063.0, 81646.0, 26612.0, 12460.0]\n",
      "1771/1771 [==============================] - 22s 13ms/step\n",
      "1771/1771 [==============================] - 24s 13ms/step - loss: 0.1066 - accuracy: 0.8673 - true_negatives: 39624.0000 - true_positives: 9503.0000 - false_negatives: 5215.0000 - false_positives: 2300.0000\n",
      "[0.1066058948636055, 0.8673245906829834, 39624.0, 9503.0, 5215.0, 2300.0]\n"
     ]
    }
   ],
   "source": [
    "pred_y_train = model.predict(X_train)\n",
    "print(model.evaluate(X_train, y_train))\n",
    "\n",
    "pred_y_test = model.predict(X_test)\n",
    "print(model.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_org = pd.read_csv(r'data/train_aug.csv')\n",
    "train_org = train_org.dropna()\n",
    "test_org = pd.read_csv(r'data/test_aug.csv')\n",
    "train_org['pred'] = np.where(pred_y_train > 0.5, 1, 0)\n",
    "test_org['pred'] = np.where(pred_y_test > 0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/96/cf/a714a655266229b51eb2bda117f15275f12457887f165f3c1cc58ab502f1/scikit_learn-1.3.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached scikit_learn-1.3.0-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\anaconda3\\envs\\mzp\\lib\\site-packages (from scikit-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\user\\anaconda3\\envs\\mzp\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.1.1 from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Using cached scikit_learn-1.3.0-cp310-cp310-win_amd64.whl (9.2 MB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.0 threadpoolctl-3.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\anaconda3\\envs\\mzp\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\anaconda3\\envs\\mzp\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.79      0.77    333523\n",
      "           1       0.24      0.21      0.23    108258\n",
      "\n",
      "    accuracy                           0.65    441781\n",
      "   macro avg       0.50      0.50      0.50    441781\n",
      "weighted avg       0.63      0.65      0.64    441781\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91     41924\n",
      "           1       0.81      0.65      0.72     14718\n",
      "\n",
      "    accuracy                           0.87     56642\n",
      "   macro avg       0.84      0.80      0.82     56642\n",
      "weighted avg       0.86      0.87      0.86     56642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(train_org['immoral'], train_org['pred']))\n",
    "print(classification_report(test_org['immoral'], test_org['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8909727 ],\n",
       "       [0.9999909 ],\n",
       "       [0.03926441],\n",
       "       ...,\n",
       "       [0.42057064],\n",
       "       [1.        ],\n",
       "       [0.00595657]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mzp",
   "language": "python",
   "name": "mzp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
